{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import utils \n",
    "import itertools\n",
    "import random as rand\n",
    "import math\n",
    "import copy\n",
    "sys.path.append('../')\n",
    "sys.path.insert(1, '../exp')\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import cross_validate, train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "#PostgreSQL\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# sqlgen\n",
    "import sqlgen\n",
    "from sqlgen import study, trial\n",
    "from sqlgen.samplers import tpe, random\n",
    "\n",
    "# graph\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "\n",
    "# featuretools\n",
    "import featuretools as ft\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345699\n",
      "Train true label: 799 Train false label: 801\n",
      "Test true label: 208 Test false label: 192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>age_range</th>\n",
       "      <th>gender</th>\n",
       "      <th>item_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>brand_id</th>\n",
       "      <th>action_type</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>213820</td>\n",
       "      <td>2066</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>195</td>\n",
       "      <td>2286</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>318509</td>\n",
       "      <td>4143</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>323850</td>\n",
       "      <td>361</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>380935</td>\n",
       "      <td>3191</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  merchant_id  age_range  gender  item_id  cat_id  brand_id  \\\n",
       "0   213820         2066          2       0        5       6         7   \n",
       "1      195         2286          5       0       16       8         5   \n",
       "2   318509         4143          6       0        8       6        11   \n",
       "3   323850          361          6       0        6       0        13   \n",
       "4   380935         3191          3       0        4       8        12   \n",
       "\n",
       "   action_type  label  \n",
       "0            0      0  \n",
       "1            1      1  \n",
       "2            0      0  \n",
       "3            0      1  \n",
       "4            0      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../exp_data/Tmall/\"\n",
    "train_data = pd.read_csv(os.path.join(path, \"train_data.csv\"))\n",
    "test_data = pd.read_csv(os.path.join(path, \"test_data.csv\"))\n",
    "user_log = pd.read_csv(os.path.join(path, \"user_log.csv\"))\n",
    "\n",
    "train_data = train_data.drop(train_data.columns[0], axis=1)\n",
    "test_data = test_data.drop(test_data.columns[0], axis=1)\n",
    "user_log = user_log.drop(user_log.columns[0], axis=1)\n",
    "# data = pd.concat([train_data, test_data])\n",
    "print(len(user_log))\n",
    "print(\"Train true label:\", len(train_data[train_data['label']==1]), 'Train false label:', len(train_data[train_data['label']==0]))\n",
    "print(\"Test true label:\", len(test_data[test_data['label']==1]), 'Test false label:', len(test_data[test_data['label']==0]))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Store table in PostragSQL\n",
    "Store train_data to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = utils.store_tmall(user_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Feature Generation & Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_score, clf = utils.model_tmall(train_data)\n",
    "test_pred = clf.predict(test_data.drop(columns='label', axis=1))\n",
    "test_score = metrics.roc_auc_score(test_data['label'], test_pred)\n",
    "print(valid_score, test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global copy_train_data, copy_test_data, train_data, test_data\n",
    "global trail_auc_store, trail_mi_store, train_auc_store\n",
    "\n",
    "trail_auc_store = []\n",
    "trail_mi_store = []\n",
    "train_auc_store = []\n",
    "copy_train_data = train_data\n",
    "copy_test_data = test_data\n",
    "\n",
    "global random_args_log, tpe_args_log\n",
    "global random_user_train_log, tpe_user_train_log\n",
    "\n",
    "random_args_log = {}\n",
    "tpe_args_log = {}\n",
    "random_user_train_log = {}\n",
    "tpe_user_train_log = {}\n",
    "\n",
    "global eva_global, seed_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_x is to store top features \n",
    "global res_x \n",
    "res_x = list()\n",
    "\n",
    "aggregation = ['SUM', 'MIN', 'MAX', 'COUNT', 'AVG'] \n",
    "m_attr = ['merchant_id', 'item_id', 'brand_id', 'cat_id']\n",
    "categorical = ['gender', 'age_range', 'action_type']\n",
    "numerical = ['time_stamp']\n",
    "attributes = categorical + numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_user_train(args, train_data):  \n",
    "    agg, m, lb_day, ub_day, w_cat_value_0, w_cat_value_1, w_cat_value_2 = args \n",
    "    agg = aggregation[agg]\n",
    "    m = m_attr[m]\n",
    "    \n",
    "    w_cat_value_temp = [w_cat_value_0, w_cat_value_1, w_cat_value_2] #, w_cat_value_3]\n",
    "    w_cat = []\n",
    "    w_cat_value = []\n",
    "\n",
    "    for i in range(len(categorical)):\n",
    "        if w_cat_value_temp[i] != -1:\n",
    "            w_cat.append(categorical[i])\n",
    "            w_cat_value.append(w_cat_value_temp[i])\n",
    "            \n",
    "    sql_output = utils.generate_feature_in_small_space_tmall(engine, agg, m, lb_day, ub_day, w_cat, w_cat_value) \n",
    "    w_cat_str = \"_\".join(str(w_cat[x])+'='+str(w_cat_value[x]) for x in range(len(w_cat)))\n",
    "    new_feature = pd.DataFrame(sql_output, columns = ['user_id', agg+'('+m+')_'+w_cat_str+'_'+'lb_day='+str(lb_day)+\"_ub_day=\"+str(ub_day)]) #+'='+g_cat\n",
    "    new_feature = new_feature.astype('float')\n",
    "    new_train_data = train_data.merge(new_feature, how='left')\n",
    "    \n",
    "    return new_train_data, args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an objective function\n",
    "def objective(trial):\n",
    "    agg = trial.suggest_categorical('agg', np.array([i for i in range(len(aggregation))]))\n",
    "    m = trial.suggest_categorical('m', np.array([i for i in range(len(m_attr))]))  \n",
    "    lb_day = trial.suggest_categorical('lb_day', sorted_time[0: math.ceil(len(sorted_time)/2)])\n",
    "    ub_day = trial.suggest_categorical('ub_day', sorted_time[math.ceil(len(sorted_time)/2)+1: -1])\n",
    "    w_cat_value_0 = trial.suggest_categorical('w_cat_value_0', np.append(user_log[categorical[0]].unique(), [-1], 0))\n",
    "    w_cat_value_1 = trial.suggest_categorical('w_cat_value_1', np.append(user_log[categorical[1]].unique(), [-1], 0))\n",
    "    w_cat_value_2 = trial.suggest_categorical('w_cat_value_2', np.append(user_log[categorical[2]].unique(), [-1], 0))\n",
    "\n",
    "    args = [agg, m, lb_day, ub_day, w_cat_value_0, w_cat_value_1, w_cat_value_2] #, w_cat_value_3]\n",
    "    new_train_data, args = update_user_train(args, train_data)\n",
    "    score, _ = utils.model_tmall(new_train_data)\n",
    "    \n",
    "    for x in res_x:\n",
    "        if args == x:\n",
    "            score -= 0.2\n",
    "            break\n",
    "        \n",
    "    print(args, \" auc: \", score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a NEW function with mutual information\n",
    "\n",
    "def mi(trial):\n",
    "    agg = trial.suggest_categorical('agg', np.array([i for i in range(len(aggregation))]))\n",
    "    m = trial.suggest_categorical('m', np.array([i for i in range(len(m_attr))]))  \n",
    "    lb_day = trial.suggest_categorical('lb_day', sorted_time[0: math.ceil(len(sorted_time)/2)])\n",
    "    ub_day = trial.suggest_categorical('ub_day', sorted_time[math.ceil(len(sorted_time)/2)+1: -1])\n",
    "    w_cat_value_0 = trial.suggest_categorical('w_cat_value_0', np.append(user_log[categorical[0]].unique(), [-1], 0))\n",
    "    w_cat_value_1 = trial.suggest_categorical('w_cat_value_1', np.append(user_log[categorical[1]].unique(), [-1], 0))\n",
    "    w_cat_value_2 = trial.suggest_categorical('w_cat_value_2', np.append(user_log[categorical[2]].unique(), [-1], 0))\n",
    "\n",
    "    args = [agg, m, lb_day, ub_day, w_cat_value_0, w_cat_value_1, w_cat_value_2] #, w_cat_value_3]\n",
    "    new_train_data, args = update_user_train(args, train_data)\n",
    "\n",
    "    mi_matrix = mutual_info_classif(new_train_data.fillna(0), new_train_data['label'], random_state=0)\n",
    "    mi_score = mi_matrix[-1]\n",
    "    \n",
    "    for x in res_x:\n",
    "        if args == x:\n",
    "            mi_score = 0\n",
    "            break\n",
    "            \n",
    "    print(args, \"MI:\", mi_score)\n",
    "    return mi_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sampling \n",
    "def evaluate(trial_arr, mi_log, train_size):\n",
    "    x_features = pd.DataFrame()\n",
    "    x_features['index'] = range(0, len(mi_log))\n",
    "    x_features['mi'] = mi_log\n",
    "    x_features['agg'] = [trial_arr[i]['agg'] for i in range(len(trial_arr))]\n",
    "    x_features['m'] = [trial_arr[i]['m'] for i in range(len(trial_arr))]\n",
    "    x_features['lb_day'] = [trial_arr[i]['lb_day'] for i in range(len(trial_arr))]\n",
    "    x_features['ub_day'] = [trial_arr[i]['ub_day'] for i in range(len(trial_arr))]\n",
    "    x_features['w_cat_value_0'] = [trial_arr[i]['w_cat_value_0'] for i in range(len(trial_arr))]\n",
    "    x_features['w_cat_value_1'] = [trial_arr[i]['w_cat_value_1'] for i in range(len(trial_arr))]\n",
    "    x_features['w_cat_value_2'] = [trial_arr[i]['w_cat_value_2'] for i in range(len(trial_arr))]\n",
    "\n",
    "    print(x_features.shape)\n",
    "    x_train, x_test = train_test_split(x_features, train_size=200, random_state=0)\n",
    "    \n",
    "    y_acc = []\n",
    "    for index, feature in x_train.iterrows():\n",
    "        args = [int(feature['agg']),\n",
    "               int(feature['m']),\n",
    "               int(feature['lb_day']),\n",
    "               int(feature['ub_day']),\n",
    "               int(feature['w_cat_value_0']),\n",
    "               int(feature['w_cat_value_1']),\n",
    "               int(feature['w_cat_value_2'])] \n",
    "        new_train_data, args = update_user_train(args, train_data)\n",
    "        \n",
    "        y_acc.append(utils.model_tmall(new_train_data)[0])\n",
    "\n",
    "    y_train = pd.DataFrame()\n",
    "    y_train['index'] = x_train['index']\n",
    "    y_train['label'] = y_acc\n",
    "\n",
    "    global clf\n",
    "    clf =  RandomForestRegressor(random_state=0) #LinearRegression()\n",
    "    clf.fit(x_train[['mi']], y_train['label'])\n",
    "    \n",
    "    global estimated_accuracy\n",
    "    estimated_accuracy = clf.predict(x_features[['mi']])\n",
    "    predict_y = pd.DataFrame()\n",
    "    predict_y['index'] = range(0, len(mi_log))\n",
    "    predict_y['label'] = estimated_accuracy\n",
    "    \n",
    "    for index, row in y_train.iterrows():\n",
    "        predict_y.loc[predict_y['index'] == row['index'], 'label'] = row['label']\n",
    "    \n",
    "    best_trail_index = y_train.nlargest(1, ['label']).index[0]\n",
    "    train_auc_store.append(predict_y['label'].to_numpy())\n",
    "    return predict_y['label'].to_numpy(), best_trail_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 5\n",
    "\n",
    "n_calls = [1500]\n",
    "random_state_arr = [0] \n",
    "\n",
    "for n_call in n_calls:\n",
    "    tpe_args_log[n_call] = {}\n",
    "    tpe_user_train_log[n_call] = {}\n",
    "    for seed in random_state_arr:\n",
    "        tpe_args_log[n_call][seed] = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLGEN - Opt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eva_store_optuna = []\n",
    "\n",
    "start = time.time()\n",
    "for eva in n_calls:\n",
    "    print(\"trails:\", eva)\n",
    "    eva_global = eva\n",
    "    seed_store = []\n",
    "    for seed in random_state_arr:\n",
    "        seed_global = seed\n",
    "        global train_data, res_x \n",
    "        res_optuna_lst = []\n",
    "        res_optuna_fun = []\n",
    "        res_x = list()\n",
    "        train_data = copy_train_data\n",
    "        print(\"seed:\", seed)\n",
    "\n",
    "        for i in range(top):\n",
    "            tpe_result = study.create_study(\n",
    "                direction=\"maximize\", \n",
    "                sampler=tpe.TPESampler(n_startup_trials=20, seed=seed)) #\n",
    "            tpe_result.optimize(objective, n_trials=eva, \n",
    "                                mi_initializer=mi, evaluate=evaluate, mi_init_trails=1000, train_size=100)\n",
    "            \n",
    "            best_trial = []\n",
    "            for key, value in tpe_result.best_trial.params.items():\n",
    "                best_trial.append(value)\n",
    "            res_x.append(best_trial)\n",
    "\n",
    "            train_data, args = update_user_train(best_trial, train_data)\n",
    "            res_optuna_lst.append(tpe_result.best_trial.params)\n",
    "            res_optuna_fun.append(tpe_result.best_value)\n",
    "\n",
    "            print(tpe_result.best_trial.params, tpe_result.best_value)\n",
    "            print(train_data.shape)\n",
    "            tpe_args_log[eva][seed].append(args)\n",
    "        seed_store.append(res_optuna_fun)\n",
    "        tpe_user_train_log[eva][seed] = train_data\n",
    "        \n",
    "    eva_store_optuna.append(seed_store)\n",
    "end = time.time()\n",
    "print(\"Time:\", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Test ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_tmall(user_train):\n",
    "    y = user_train['label']\n",
    "    y = y.to_frame()\n",
    "    X = user_train.drop(['label'], axis=1)\n",
    "    X = X.fillna(0)\n",
    "\n",
    "    clf = xgb.XGBRegressor(random_state=0)\n",
    "    auc = cross_validate(clf, X, y, cv=5,scoring=('roc_auc'), return_train_score=True, n_jobs=-1, return_estimator=True)\n",
    "    valid_auc = auc['test_score'].mean()\n",
    "    train_auc = auc['train_score'].mean()\n",
    "\n",
    "    return train_auc, valid_auc, auc['estimator'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_auc = {}\n",
    "train_auc = {}\n",
    "valid_auc = {}\n",
    "for n_calls in tpe_args_log:\n",
    "    for seed in tpe_args_log[n_calls]:\n",
    "        test_auc[seed] = []\n",
    "        train_auc[seed] = []\n",
    "        valid_auc[seed] = []\n",
    "        train_data = copy_train_data\n",
    "        test_data = copy_test_data\n",
    "        \n",
    "        # Default AUC without new features\n",
    "        train, valid, clf = model_tmall(train_data)\n",
    "        train_auc[seed].append(train)\n",
    "        valid_auc[seed].append(valid)\n",
    "        test_pred = clf.predict(test_data.drop(columns='label', axis=1))\n",
    "        test_auc[seed].append(metrics.roc_auc_score(test_data['label'], test_pred))\n",
    "        for args in tpe_args_log[n_calls][seed]:\n",
    "            test_data, config = update_user_train(args, test_data)\n",
    "            train_data, config = update_user_train(args, train_data)\n",
    "            train, valid, clf = model_tmall(train_data)\n",
    "            train_auc[seed].append(train)\n",
    "            valid_auc[seed].append(valid)\n",
    "            \n",
    "            test_data = test_data.fillna(0)\n",
    "            test_pred = clf.predict(test_data.drop(columns='label', axis=1))\n",
    "            test_auc[seed].append(metrics.roc_auc_score(test_data['label'], test_pred))\n",
    "train_auc, valid_auc, test_auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_fg",
   "language": "python",
   "name": "auto_fg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
