{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'optuna'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bef3cea2bdd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# sqlgen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msqlgen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msqlgen\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msqlgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtpe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/SQLGen/sqlgen/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msqlgen\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstudy\u001b[0m  \u001b[0;31m# NOQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msqlgen\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrial\u001b[0m  \u001b[0;31m# NOQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msqlgen\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisualization\u001b[0m  \u001b[0;31m# NOQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msqlgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrialPruned\u001b[0m  \u001b[0;31m# NOQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/SQLGen/sqlgen/visualization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msqlgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontour\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_contour\u001b[0m  \u001b[0;31m# NOQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msqlgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_values\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_intermediate_values\u001b[0m  \u001b[0;31m# NOQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msqlgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimization_history\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_optimization_history\u001b[0m  \u001b[0;31m# NOQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msqlgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_coordinate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_parallel_coordinate\u001b[0m  \u001b[0;31m# NOQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msqlgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_slice\u001b[0m  \u001b[0;31m# NOQA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/SQLGen/sqlgen/visualization/intermediate_values.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_logger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTrialState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moptuna\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtype_checking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_check_plotly_availability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optuna'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import utils \n",
    "import itertools\n",
    "import random as rand\n",
    "import math\n",
    "import copy\n",
    "sys.path.append('../')\n",
    "sys.path.insert(1, '../exp')\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import cross_validate, train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "#PostgreSQL\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# sqlgen\n",
    "import sqlgen\n",
    "from sqlgen import study, trial\n",
    "from sqlgen.samplers import tpe, random\n",
    "from sqlgen.importance._fanova import FanovaImportanceEvaluator\n",
    "from sqlgen.importance._mean_decrease_impurity import MeanDecreaseImpurityImportanceEvaluator\n",
    "\n",
    "# graph\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly\n",
    "\n",
    "# featuretools\n",
    "import featuretools as ft\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Load data & Join Table\n",
    "\n",
    "Since we only need two tables: 'User' table and 'Log' table, we will concate three tables(ie. orders, order_products and products) together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"../exp_data/Instacart/\"\n",
    "order_products = pd.read_csv(os.path.join(path, \"order_products__prior.csv\"))\n",
    "orders = pd.read_csv(os.path.join(path, \"orders.csv\"))\n",
    "departments = pd.read_csv(os.path.join(path, \"departments.csv\"))\n",
    "products = pd.read_csv(os.path.join(path, \"products.csv\"))\n",
    "\n",
    "user = pd.DataFrame()\n",
    "user['user_id'] = orders[\"user_id\"]\n",
    "user = user.drop_duplicates(keep = 'first', inplace=False)\n",
    "\n",
    "log = orders.merge(order_products).merge(products).sort_values(by=['user_id', 'order_number'])\n",
    "print(\"number of users:\", len(user), \", number of logs:\", len(log))\n",
    "\n",
    "orders_train = orders[orders['eval_set']=='train']\n",
    "global user_train\n",
    "user_train = pd.DataFrame()\n",
    "user_train['user_id'] = orders_train['user_id']\n",
    "log_train = user_train.merge(orders).merge(order_products).merge(products).sort_values(by=['user_id', 'order_number'])\n",
    "print(\"training users:\", len(user_train), \"training data:\", len(log_train))\n",
    "\n",
    "orders_test = orders[orders['eval_set']=='test']\n",
    "user_test = pd.DataFrame()\n",
    "user_test['user_id'] = orders_test['user_id']\n",
    "log_test = user_test.merge(orders).merge(order_products).merge(products).sort_values(by=['user_id', 'order_number'])\n",
    "print(\"test users:\", len(user_test), \"test data:\", len(log_test))\n",
    "\n",
    "log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert product_name into 0/1 \n",
    "Because 'product_name' attribute has too many distinct values, here we simplify the attribute with only two values. \n",
    "\n",
    "If the product name contains 'Banana', then product_name=True, otherwise, Flase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_product_name = log_train['product_name'].str.contains(r'Banana')\n",
    "log_train = log_train.drop('product_name', 1)\n",
    "log_train['product_name'] = new_product_name\n",
    "log_train = log_train[['user_id', 'order_id', 'product_id', 'eval_set', 'order_number', \n",
    "                      'order_dow', 'order_hour_of_day', 'days_since_prior_order', 'add_to_cart_order', \n",
    "                       'reordered', 'aisle_id', 'department_id', 'product_name']]\n",
    "log_train['product_name'] = log_train['product_name'].astype(int)\n",
    "log_train = log_train.fillna(0)\n",
    "print(\"# Banana:\", len(log_train[log_train['product_name']==True]), \"# non-Banana:\", len(log_train[log_train['product_name']==False]))\n",
    "log_train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Make Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training = log_train[log_train['eval_set']=='train']\n",
    "has_Banana = {}\n",
    "for i, row in training.iterrows():\n",
    "    if row[-1] == True:\n",
    "        has_Banana.update({row[0]: 1})\n",
    "print('# of users buy banana', len(has_Banana))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_train['label'] = np.nan\n",
    "for item in has_Banana.items():\n",
    "    user_train.loc[user_train['user_id'] == item[0], 'label'] = 1\n",
    "\n",
    "# fillna with 0 \n",
    "user_train = user_train.fillna(0)\n",
    "print(\"neg samples:\", len(user_train[user_train['label']==0]))\n",
    "print(\"pos samples:\", len(user_train[user_train['label']==1]))\n",
    "user_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        user_train.drop(['label'], axis=1), user_train['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "train_data = X_train\n",
    "train_data['label'] = y_train\n",
    "test_data = X_test\n",
    "test_data['label'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global copy_train_data, copy_test_data, train_data, test_data\n",
    "global trail_auc_store, trail_mi_store, train_auc_store\n",
    "\n",
    "trail_auc_store = []\n",
    "trail_mi_store = []\n",
    "trail_args_store = []\n",
    "train_auc_store = []\n",
    "\n",
    "global random_args_log, tpe_args_log\n",
    "global random_user_train_log, tpe_user_train_log\n",
    "\n",
    "random_args_log = {}\n",
    "tpe_args_log = {}\n",
    "random_user_train_log = {}\n",
    "tpe_user_train_log = {}\n",
    "\n",
    "global eva_global, seed_global"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Store table in PostragSQL\n",
    "Store log_train to databse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "engine = utils.store_instacart(log_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Feature Generation & Machine Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_train[\"index\"] = log_train[\"user_id\"].astype(str) + \"_\" + log_train[\"order_id\"].astype(str) + \"_\" + log_train[\"product_id\"].astype(str)\n",
    "log_train = log_train[[\"index\", \"user_id\", \"order_id\", \"product_id\", \"product_name\", \"department_id\", \"order_dow\", \n",
    "                     \"days_since_prior_order\"]]\n",
    "\n",
    "log_train_copy = copy.deepcopy(log_train)\n",
    "log_train_copy = log_train_copy.rename({'index': 'index_copy'}, axis=1)\n",
    "log_train_copy['unique_id'] = range(0, len(log_train_copy))\n",
    "log_train_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_train_vtypes = {\n",
    "    \"index\": ft.variable_types.Index,\n",
    "    \"user_id\": ft.variable_types.Id,\n",
    "    \"order_id\": ft.variable_types.Id,\n",
    "    \"product_id\": ft.variable_types.Id,\n",
    "    \"order_dow\": ft.variable_types.Numeric,\n",
    "    \"days_since_prior_order\": ft.variable_types.TimeIndex,\n",
    "    \"department_id\": ft.variable_types.Numeric,\n",
    "    \"product_name\": ft.variable_types.Numeric\n",
    "}\n",
    "\n",
    "es = ft.EntitySet(\"instacart\")\n",
    "es.entity_from_dataframe(entity_id=\"log_train\",\n",
    "                         dataframe=log_train,\n",
    "                         index=\"index\",\n",
    "                         variable_types=log_train_vtypes)\n",
    "\n",
    "es.normalize_entity(base_entity_id=\"log_train\", new_entity_id=\"users\", index=\"user_id\")\n",
    "\n",
    "es.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "feature_matrix, features = ft.dfs(target_entity=\"users\",\n",
    "                                  agg_primitives = [\"sum\", \"min\", \"max\", \"count\", \"mean\"],\n",
    "#                                   where_primitives = [\"count\"],\n",
    "                                  trans_primitives = [],\n",
    "                                  ignore_variables={\"log_train\":[\"order_id\", \"product_id\", \"department_id\"]},\n",
    "                                  entityset=es,\n",
    "                                  verbose=True)\n",
    "end = time.time()\n",
    "print(\"Time:\", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Train & Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.merge(feature_matrix, on='user_id', how=\"left\")\n",
    "test_data = test_data.merge(feature_matrix, on='user_id', how=\"left\")\n",
    "\n",
    "copy_train_data = copy.deepcopy(train_data)\n",
    "copy_test_data = copy.deepcopy(test_data)\n",
    "\n",
    "train_data = train_data.reindex(sorted(train_data.columns), axis=1)\n",
    "test_data = test_data.reindex(sorted(test_data.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLGEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set # of evaluation of opt algo\n",
    "global res_x \n",
    "res_x = list()\n",
    "\n",
    "aggregation = ['SUM', 'MIN', 'MAX', 'COUNT', 'AVG'] \n",
    "categorical_m = ['product_name', 'order_dow']\n",
    "categorical = ['product_name', 'department_id', 'order_dow']\n",
    "numerical = ['days_since_prior_order']\n",
    "attributes = categorical + numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_user_train(args, train_data):  \n",
    "    agg, m, lb_day, ub_day, w_cat_value_0, w_cat_value_1, w_cat_value_2 = args \n",
    "    agg = aggregation[agg]\n",
    "    m = categorical_m[m]\n",
    "    \n",
    "    w_cat_value_temp = [w_cat_value_0, w_cat_value_1, w_cat_value_2]\n",
    "    w_cat = []\n",
    "    w_cat_value = []\n",
    "\n",
    "    for i in range(len(categorical)):\n",
    "        if w_cat_value_temp[i] != -1:\n",
    "            w_cat.append(categorical[i])\n",
    "            w_cat_value.append(w_cat_value_temp[i])\n",
    "            \n",
    "    sql_output = utils.generate_feature_in_small_space(engine, agg, m, lb_day, ub_day, w_cat, w_cat_value) \n",
    "    w_cat_str = \"_\".join(str(w_cat[x])+'='+str(w_cat_value[x]) for x in range(len(w_cat)))\n",
    "    new_feature = pd.DataFrame(sql_output, columns = ['user_id', agg+'('+m+')_'+w_cat_str+'_'+'lb_day='+str(lb_day)+\"_ub_day=\"+str(ub_day)]) #+'='+g_cat\n",
    "    new_feature = new_feature.astype('float')\n",
    "    new_train_data = train_data.merge(new_feature, how='left')\n",
    "    \n",
    "    return new_train_data, args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an objective function\n",
    "\n",
    "def objective(trial):\n",
    "    agg = trial.suggest_categorical('agg', np.array([i for i in range(len(aggregation))]))\n",
    "    m = trial.suggest_categorical('m', np.array([i for i in range(len(categorical_m))]))  \n",
    "    lb_day = trial.suggest_int('lb_day', 0,14)\n",
    "    ub_day = trial.suggest_int('ub_day', 15,30)\n",
    "    w_cat_value_0 = trial.suggest_categorical('w_cat_value_0', np.append(log_train[categorical[0]].unique(), [-1], 0))\n",
    "    w_cat_value_1 = trial.suggest_categorical('w_cat_value_1', np.append(log_train[categorical[1]].unique(), [-1], 0))\n",
    "    w_cat_value_2 = trial.suggest_categorical('w_cat_value_2', np.append(log_train[categorical[2]].unique(), [-1], 0))\n",
    "    \n",
    "    args = [agg, m, lb_day, ub_day, w_cat_value_0, w_cat_value_1, w_cat_value_2]\n",
    "    new_train_data, args = update_user_train(args, train_data)\n",
    "    score, _ = utils.model(new_train_data)\n",
    "    \n",
    "    for x in res_x:\n",
    "        if args == x:\n",
    "            score -= 0.2\n",
    "            break\n",
    "            \n",
    "    print(args, \" Accuracy: \", score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a NEW function with mutual information\n",
    "\n",
    "def mi(trial):\n",
    "    agg = trial.suggest_categorical('agg', np.array([i for i in range(len(aggregation))]))\n",
    "    m = trial.suggest_categorical('m', np.array([i for i in range(len(categorical_m))]))  \n",
    "    lb_day = trial.suggest_int('lb_day', 0,14)\n",
    "    ub_day = trial.suggest_int('ub_day', 15,30)\n",
    "    w_cat_value_0 = trial.suggest_categorical('w_cat_value_0', np.append(log_train[categorical[0]].unique(), [-1], 0))\n",
    "    w_cat_value_1 = trial.suggest_categorical('w_cat_value_1', np.append(log_train[categorical[1]].unique(), [-1], 0))\n",
    "    w_cat_value_2 = trial.suggest_categorical('w_cat_value_2', np.append(log_train[categorical[2]].unique(), [-1], 0))\n",
    "    \n",
    "    args = [agg, m, lb_day, ub_day, w_cat_value_0, w_cat_value_1, w_cat_value_2]\n",
    "    new_train_data, args = update_user_train(args, train_data)\n",
    "\n",
    "    mi_matrix = mutual_info_classif(new_train_data.fillna(0), new_train_data['label'], random_state=0)\n",
    "    mi_score = mi_matrix[-1]\n",
    "    \n",
    "    for x in res_x:\n",
    "        if args == x:\n",
    "            mi_score = 0\n",
    "            break\n",
    "            \n",
    "    trail_mi_store.append(mi_score)\n",
    "    trail_args_store.append(args)\n",
    "    print(args, \"MI:\", mi_score)\n",
    "    return mi_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sampling \n",
    "def evaluate(trial_arr, mi_log, train_size):\n",
    "\n",
    "    x_features = pd.DataFrame()\n",
    "    x_features['index'] = range(0, len(mi_log))\n",
    "    x_features['mi'] = mi_log\n",
    "    x_features['agg'] = [trial_arr[i]['agg'] for i in range(len(trial_arr))]\n",
    "    x_features['m'] = [trial_arr[i]['m'] for i in range(len(trial_arr))]\n",
    "    x_features['lb_day'] = [trial_arr[i]['lb_day'] for i in range(len(trial_arr))]\n",
    "    x_features['ub_day'] = [trial_arr[i]['ub_day'] for i in range(len(trial_arr))]\n",
    "    x_features['w_cat_value_0'] = [trial_arr[i]['w_cat_value_0'] for i in range(len(trial_arr))]\n",
    "    x_features['w_cat_value_1'] = [trial_arr[i]['w_cat_value_1'] for i in range(len(trial_arr))]\n",
    "    x_features['w_cat_value_2'] = [trial_arr[i]['w_cat_value_2'] for i in range(len(trial_arr))]\n",
    "    \n",
    "    print(x_features.shape)\n",
    "    x_train, x_test = train_test_split(x_features, train_size=train_size, random_state=0)\n",
    "\n",
    "    y_acc = []\n",
    "    for index, feature in x_train.iterrows():\n",
    "        args = [int(feature['agg']),\n",
    "               int(feature['m']),\n",
    "               int(feature['lb_day']),\n",
    "               int(feature['ub_day']),\n",
    "               int(feature['w_cat_value_0']),\n",
    "               int(feature['w_cat_value_1']),\n",
    "               int(feature['w_cat_value_2'])] \n",
    "        new_train_data, args = update_user_train(args, train_data)\n",
    "        \n",
    "        y_acc.append(utils.model(new_train_data)[0])\n",
    "\n",
    "    y_train = pd.DataFrame()\n",
    "    y_train['index'] = x_train['index']\n",
    "    y_train['label'] = y_acc\n",
    "    \n",
    "#     global train_auc_store\n",
    "#     train_auc_store.append(x_train)\n",
    "#     train_auc_store.append(y_train)\n",
    "    \n",
    "    global clf\n",
    "    clf = RandomForestRegressor(random_state=0)\n",
    "    clf.fit(x_train[['mi']], y_train['label'])\n",
    "    \n",
    "    global estimated_accuracy\n",
    "    estimated_accuracy = clf.predict(x_features[['mi']])\n",
    "    predict_y = pd.DataFrame()\n",
    "    predict_y['index'] = range(0, len(mi_log))\n",
    "    predict_y['label'] = estimated_accuracy\n",
    "    \n",
    "    for index, row in y_train.iterrows():\n",
    "        predict_y.loc[predict_y['index'] == row['index'], 'label'] = row['label']\n",
    "    \n",
    "    best_trail_index = y_train.nlargest(1, ['label']).index[0]\n",
    "    train_auc_store.append(predict_y['label'].to_numpy())\n",
    "    return predict_y['label'].to_numpy(), best_trail_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 5\n",
    "\n",
    "n_calls = [1500] \n",
    "random_state_arr = [0] \n",
    "train_size_arr = [100]\n",
    "all_store = []\n",
    "for n_call in n_calls:\n",
    "    tpe_args_log[n_call] = {}\n",
    "    tpe_user_train_log[n_call] = {}\n",
    "    for seed in random_state_arr:\n",
    "        tpe_args_log[n_call][seed] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQLGEN - Opt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_store_optuna = []\n",
    "train_auc_store\n",
    "start = time.time()\n",
    "\n",
    "for eva in n_calls:\n",
    "    print(\"trails:\", eva)\n",
    "    eva_global = eva\n",
    "    seed_store = []\n",
    "    for seed in random_state_arr:\n",
    "        seed_global = seed\n",
    "        global train_data, res_x \n",
    "        res_optuna_lst = []\n",
    "        res_optuna_fun = []\n",
    "        res_x = list()\n",
    "        train_data = copy_train_data\n",
    "        print(\"seed:\", seed)\n",
    "\n",
    "        for i in range(top):\n",
    "            tpe_result = study.create_study(\n",
    "                direction=\"maximize\", \n",
    "                sampler=tpe.TPESampler(n_startup_trials=20, seed=seed)) #\n",
    "            tpe_result.optimize(objective, n_trials=eva, \n",
    "                                mi_initializer=mi, evaluate=evaluate, mi_init_trails=1000, train_size=train_size)\n",
    "\n",
    "            best_trial = []\n",
    "            for key, value in tpe_result.best_trial.params.items():\n",
    "                best_trial.append(value)\n",
    "            res_x.append(best_trial)\n",
    "\n",
    "            train_data, args = update_user_train(best_trial, train_data)\n",
    "            res_optuna_lst.append(tpe_result.best_trial.params)\n",
    "            res_optuna_fun.append(tpe_result.best_value)\n",
    "\n",
    "            print(tpe_result.best_trial.params, tpe_result.best_value)\n",
    "            print(train_data.shape)\n",
    "            tpe_args_log[eva][seed].append(args)\n",
    "        seed_store.append(res_optuna_fun)\n",
    "        tpe_user_train_log[eva][seed] = train_data\n",
    "\n",
    "    eva_store_optuna.append(seed_store)\n",
    "end = time.time()\n",
    "print(\"Time:\", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_instacart(user_train):\n",
    "    y = user_train['label']\n",
    "    y = y.to_frame()\n",
    "    X = user_train.drop(['label'], axis=1)\n",
    "    X = X.fillna(0)\n",
    "\n",
    "    clf = RandomForestClassifier(random_state=0)\n",
    "    auc = cross_validate(clf, X, y, cv=5,scoring=('accuracy'), return_train_score=True, n_jobs=-1, return_estimator=True)\n",
    "    valid_auc = auc['test_score'].mean()\n",
    "    train_auc = auc['train_score'].mean()\n",
    "\n",
    "    return train_auc, valid_auc, auc['estimator'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_auc = {}\n",
    "train_auc = {}\n",
    "valid_auc = {}\n",
    "for n_calls in tpe_args_log:\n",
    "    for seed in tpe_args_log[n_calls]:\n",
    "        test_auc[seed] = []\n",
    "        train_auc[seed] = []\n",
    "        valid_auc[seed] = []\n",
    "        train_data = copy_train_data\n",
    "        test_data = copy_test_data\n",
    "        \n",
    "        # Default AUC without new features\n",
    "        train, valid, clf = model_instacart(train_data)\n",
    "        train_auc[seed].append(train)\n",
    "        valid_auc[seed].append(valid)\n",
    "        test_pred = clf.predict(test_data.drop(columns='label', axis=1))\n",
    "        test_auc[seed].append(metrics.accuracy_score(test_data['label'], test_pred))\n",
    "        for args in tpe_args_log[n_calls][seed]:\n",
    "            test_data, config = update_user_train(args, test_data)\n",
    "            train_data, config = update_user_train(args, train_data)\n",
    "            train, valid, clf = model_instacart(train_data)\n",
    "            train_auc[seed].append(train)\n",
    "            valid_auc[seed].append(valid)\n",
    "            \n",
    "            test_data = test_data.fillna(0)\n",
    "            test_pred = clf.predict(test_data.drop(columns='label', axis=1))\n",
    "            test_auc[seed].append(metrics.accuracy_score(test_data['label'], test_pred))\n",
    "train_auc, valid_auc, test_auc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_fg",
   "language": "python",
   "name": "auto_fg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
